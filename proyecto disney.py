# -*- coding: utf-8 -*-
"""Proyecto_Disney.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CrXeu9ffoViS4KvTKAJcle1qdhICd5oR
"""

import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tfm
if tfm.test.gpu_device_name():
  print (f'Se encontró el siguientr GPU: {tfm.test.gpu_device_name()}')
else:
  print('Aqui no hay GPU´s')

encodings = ['utf-8', 'ISO-8859-1', 'latin1']

for encoding in encodings:
    try:
        df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/NLP/DisneylandReviews.csv', encoding=encoding)
        print("File read successfully with encoding:", encoding)
        break
    except UnicodeDecodeError:
        print("Error reading the file with encoding:", encoding)
df.head()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(keep='first', inplace=True)

df['Reviewer_Location'].value_counts()

df['Reviewer_Location'].unique()

df['Rating'].value_counts()

df['Branch'].value_counts()

import re
import nltk
import numpy as np
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
stop_words_en = stopwords.words('english')
lemmatizer = WordNetLemmatizer()

def preprocesamiento_texto(texto):

    # haces todo el texto en minúsculas
    texto=texto.lower()

    # regex para caracteres especiales y números
    texto = re.sub(r'[^a-zA-Z\s]', '', texto)

    # tokenizar el texto
    texto_tokenized= word_tokenize(texto)

    # eliminar stopwords
    texto_no_stop = [token for token in texto_tokenized if token not in stop_words_en]

    # lematizar/unir las palabras procesadas nuevamente en una cadena
    final= [lemmatizer.lemmatize (word) for word in texto_no_stop]
    final=' '.join(final)

    return final

df['texto_limpio'] = df['Review_Text'].apply(preprocesamiento_texto)
df.head()

def sentiment(score):
    if score > 3:
        return 'Positive'
    elif score == 3:
        return 'Neutral'
    else:
        return 'Negative'

df['Sentiment'] = df['Rating'].apply(sentiment)
df.head()

sns.countplot(x=df['Rating'])
plt.show()

sns.countplot(x=df['Sentiment'])
plt.show()

pos = df[df['Sentiment'] == 'Positive'].sample(3000)
neg = df[df['Sentiment'] == 'Negative'].sample(3000)
neu = df[df['Sentiment'] == 'Neutral'].sample(3000)

df = pd.concat([pos,neg,neu],axis=0)
df.shape

embeddings = {}

with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/glove.6B/glove.6B.300d.txt', encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        vectors = np.asarray(values[1:])
        embeddings[word] = vectors

def vectorize(text):
    vector_size = 300
    texto = text.lower()
    texto = re.sub(r'[^a-zA-Z0-9\s]', '',texto)
    texto = word_tokenize(texto)
    texto = [palabra for palabra in texto if palabra not in stop_words_en]
    texto = [lemmatizer.lemmatize(palabra)for palabra in texto]
    vector = np.zeros(vector_size)
    for palabra in texto:
        if palabra in embeddings:
            vector = vector + embeddings[palabra].astype('float')
        else:
            print(f"No hay un embedding para la palabra {palabra}. Omitiendo...")
    vector = vector.reshape(1,-1)[0]
    return vector

df['vector']=df['Review_Text'].apply(vectorize)
df.head(10)

mapper={
    'Negative':0,
    'Neutral':1,
    'Positive':2,
}

df['Sentiment']= df['Sentiment'].map(mapper)
df.head()

from sklearn.model_selection import train_test_split

X_cnn = np.array(df['vector'].tolist())
y_cnn = df['Sentiment']

X_train_cnn,X_test_cnn,y_train_cnn,y_test_cnn = train_test_split(X_cnn,
                                                 y_cnn,
                                                 train_size=0.75,
                                                 random_state=101,
                                                 stratify=y_cnn)

y_test_cnn.shape

from sklearn.preprocessing import MinMaxScaler

scaler_cnn = MinMaxScaler()
X_train_cnn = scaler_cnn.fit_transform(X_train_cnn)
X_test_cnn = scaler_cnn.transform(X_test_cnn)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

early_stopping_cnn= EarlyStopping(monitor='val_loss',
                              patience=7,
                              restore_best_weights=True)

y_train_cnn

y_train_cnn=to_categorical(y_train_cnn,3)
y_test_cnn=to_categorical(y_test_cnn,3)

y_train_cnn.shape

y_train_cnn

X_train_cnn= X_train_cnn.reshape(X_train_cnn.shape[0], X_train_cnn.shape[1],1)
X_test_cnn= X_test_cnn.reshape(X_test_cnn.shape[0], X_test_cnn.shape[1],1)

model_cnn=Sequential()
model_cnn.add(Conv1D(32,2, activation='relu', input_shape=X_train_cnn[0].shape))
model_cnn.add(MaxPooling1D(2))
model_cnn.add(Flatten())
model_cnn.add(Dense(64, activation='relu'))
model_cnn.add(Dropout(0.35))
model_cnn.add(Dense(3, activation='softmax'))

model_cnn.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history_cnn=model_cnn.fit(X_train_cnn,
                  y_train_cnn,
                  epochs=100,
                  validation_data=(X_test_cnn, y_test_cnn),
                  callbacks=[early_stopping_cnn])

metrics_cnn=pd.DataFrame(history_cnn.history)
metrics_cnn.head()

import seaborn as sns
sns.lineplot(data=metrics_cnn[['loss', 'val_loss']])

sns.lineplot(data=metrics_cnn[['accuracy', 'val_accuracy']])

y_pred_cnn=model_cnn.predict(X_test_cnn)

y_pred_cnn=np.argmax(model_cnn.predict(X_test_cnn), axis=-1)
y_pred_cnn

y_test_cnn=np.argmax(y_test_cnn, axis=-1)

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test_cnn, y_pred_cnn))

sns.heatmap(confusion_matrix(y_test_cnn, y_pred_cnn), annot=True, fmt='.0f')

def predict(text):
  text=vectorize(text)
  text=text.reshape(1,-1)
  text=scaler_cnn.transform(text)
  text=text.reshape(1,300,1)
  text=model_cnn.predict(text)
  print(f'''
    CATEGORY.             PROBABILITY
    _____________________________________
    Negative:             {text[0][0]}
    Neutral:              {text[0][1]}
    Positive:             {text[0][2]}

  ''')

neg='''
Disney Paris was a complete disappointment. The park was overcrowded, and the staff seemed indifferent to guests' experiences. The attractions were outdated and in desperate need of maintenance. The food was overpriced and of poor quality. Overall, a regrettable waste of time and money.
'''

predict(neg)

neutral='''
Disney Paris offered a mixed experience. The park had a diverse range of attractions catering to various age groups. While some staff members were helpful, others seemed disengaged. The park's cleanliness was acceptable, but there were moments of overcrowding. Food options were decent, though prices were on the higher side. Overall, a visit to Disney Paris may provide a moderate entertainment value depending on individual preferences.
'''
predict(neutral)

positive='''
Disney Paris provided a magical and enchanting experience. The park's attention to detail in recreating beloved Disney worlds was impressive. Staff members were friendly and went out of their way to enhance the guest experience. The attractions were well-maintained, and the entertainment options were diverse and captivating. The park's cleanliness and organization contributed to a seamless and enjoyable visit. From thrilling rides to charming character interactions, Disney Paris delivered a truly enchanting and memorable adventure for visitors of all ages.
'''

predict(positive)

import joblib
joblib.dump(model_cnn, 'modelo_cnn_chido.pkl')

from google.colab import files
# Descargar el archivo
files.download('modelo_cnn_chido.pkl')

# df = df.reset_index(drop=True)

# #FNNs
# from sklearn.model_selection import train_test_split

# X_fnn = df['vector']
# X_fnn = np.concatenate(X_fnn, axis=0).reshape(-1,300)
# y_fnn = df['Sentiment']

# X_train_fnn,X_test_fnn,y_train_fnn,y_test_fnn = train_test_split(X_fnn,
#                                                  y_fnn,
#                                                  train_size=0.80,
#                                                  random_state=10)

# df['Sentiment'].value_counts().plot(kind='bar')

# np.min(X_train_fnn)

# from sklearn.preprocessing import MinMaxScaler

# scaler_fnn= MinMaxScaler()

# X_train_fnn=scaler_fnn.fit_transform(X_train_fnn)
# X_test_fnn= scaler_fnn.transform(X_test_fnn)

# np.min(X_train_fnn)

# from keras.utils import to_categorical

# y_train_fnn = to_categorical(y_train_fnn, num_classes=3)
# y_test_fnn = to_categorical(y_test_fnn, num_classes=3)

# y_train_fnn

# from tensorflow import keras
# from tensorflow.keras import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.callbacks import EarlyStopping

# early_stopping_fnn= EarlyStopping(monitor='val_loss',
#                                   patience=8,
#                                   restore_best_weights=True)

# model_fnn= Sequential()
# model_fnn.add(Dense(units=130, activation='relu', input_dim=X_fnn.shape[1]))
# model_fnn.add(Dense(units=130, activation='relu'))
# model_fnn.add(Dropout(0.2))
# model_fnn.add(Dense(units=130, activation='relu'))
# model_fnn.add(Dropout(0.3))
# model_fnn.add(Dense(units=3, activation='softmax'))

# model_fnn.compile(optimizer=Adam(learning_rate=0.001),
#               loss='categorical_crossentropy',
#               metrics=['accuracy'])

# history_fnn= model_fnn.fit(X_train_fnn,
#                    y_train_fnn,
#                    batch_size=4,
#                    epochs=100,
#                    validation_data=(X_test_fnn, y_test_fnn),
#                    callbacks=[early_stopping_fnn])

# metrics_fnn= pd.DataFrame(history_fnn.history)
# metrics_fnn.head()

# import seaborn as sns
# sns.lineplot(data=metrics_fnn[['loss', 'val_loss']])

# sns.lineplot(data=metrics_fnn[['accuracy', 'val_accuracy']])

# y_pred_fnn=np.argmax(model_fnn.predict(X_test_fnn), axis=-1)
# y_pred_fnn

# y_test_fnn=np.argmax(y_test_fnn, axis=-1)
# print(y_test_fnn.shape)

# umbral= 0.5
# y_pred_fnn= np.where(y_pred_fnn>umbral, 1,0)
# y_pred_fnn

# from sklearn.metrics import classification_report, confusion_matrix
# print(classification_report(y_test_fnn, y_pred_fnn))

# sns.heatmap(confusion_matrix(y_test_fnn, y_pred_fnn), annot=True, fmt='.0f')